# dwtools
```{r init.tech, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE,comment="#>",cache=FALSE)
# knitr::knit("dwtools/README.Rmd","dwtools/README.md")
```
**Current version: 0.8.3.5**  

Data Warehouse related functions. Handy wrappers for extraction, loading, denormalization, normalization. Data exploration tools. Additionally [data.table](https://github.com/Rdatatable/data.table) *Nth key* feature, timing+logging and more.  
See below for core functions in the package.  
Report issues on github.

## Installation
```{r installation, eval=FALSE}
devtools::install_github("jangorecki/dwtools")
```

## Core functions
```{r init, results='hide'}
library(dwtools)
```

### dw.populate
Not core function but it will populate data for the next examples.  
```{r dw.populate}
X = dw.populate(scenario="star")
SALES = X$SALES
GEOGRAPHY = X$GEOGRAPHY
head(SALES)
head(GEOGRAPHY)
```

### db
Function provides simple database interface. Designed to use in data.table chaining.  
It handles DBI drivers (tested on Postgres and SQLite), RODBC (any odbc connection, not yet tested) and csv files as tables.  
In ETL terms where `data.table` serves as **Transformation** layer, the dwtools `db` function serves **Extraction** and **Loading** layers.  
```{r db, results='hide'}
# setup db connections
library(RSQLite) # install.packages("RSQLite")
sqlite1 = list(drvName="SQLite",dbname="sqlite1.db",conn=dbConnect(SQLite(), dbname="sqlite1.db"))
options("dwtools.db.conns" = list(sqlite1=sqlite1, csv1=list(drvName="csv")))

# write to db (default connection)
db(SALES,"sales_table")
# read from from db
db("sales_table")
# query from db # not supported for csv driver
db("SELECT * FROM sales_table")
# send to db # not supported for csv driver
db("DROP TABLE sales_table")

# write geography data.table into multiple connections
db(GEOGRAPHY,"geography",c("sqlite1","csv1"))
# lookup from db and setkey
db("geography",key="geog_code")
# use data.table chaining
SALES[,.(amount=sum(amount),value=sum(value)),keyby=list(geog_code,time_code) # aggr to geog_code, time_code
      ][,db("geography",key="geog_code")[.SD] # lookup from sqlite1
        ][,.(amount=sum(amount),value=sum(value)),keyby=list(geog_division_name,time_code) # aggr to division_code, time_code
          ]
SALES[,.SD,keyby=list(geog_code) # setkey
      ][db("geography","csv1",key=1 # join to geography from csv file, setkey on first column
           )[geog_division_name=="East South Central" # filter geography to one division_name
             ], nomatch=0 # inner join
        ]
```
`db` function accepts vector of sql statements / table names to allow batch processing.  
All `db` function calls can be logged by via argument `timing=TRUE`, or automatically (and not only for `db`) via `options("dwtools.timing"=TRUE)`. See `?timing`.    
In case of tables migration see `?dbCopy`.

### joinbyv
Batch join multiple tables into one master table.  
Denormalization of star schema and snowflake schema to flat fact table.  
```{r joinbyv}
sapply(X, nrow) # nrow of each tbl in star schema
# denormalize 
DT = joinbyv(
  master = X$SALES,
  join = list(customer = X$CUSTOMER,
              product = X$PRODUCT,
              geography = X$GEOGRAPHY,
              time = X$TIME,
              currency = X$CURRENCY),
  col.subset = list(c("cust_active"),
                    c("prod_group_name","prod_family_name"),
                    c("geog_region_name"),
                    c("time_month_name"),
                    NULL)
  )
print(names(DT))
```

### build_hierarchy
Takes single dataset on input and detects hierarchies by cardinality of unique groupings of all possible variable pairs. Returns star schema: one fact table and multiple dimension tables.
```{r build_hierarchy}
X = dw.populate(N=1e5, scenario="star")
DT <- joinbyv(X$SALES, join=list(X$CURRENCY, X$GEOGRAPHY))
names(DT)
dw <- build_hierarchy(DT, factname="fact_sales")
sapply(dw$tables,ncol)
sapply(dw$tables,nrow)
```

```{r shinyBI_clean, echo=FALSE}
suppressWarnings(rm(x,dw))
```

### shinyBI
Early version of hierarchical data BI app.
```{r shinyBI, eval=FALSE}
shiny::runApp(system.file("shinyBI", package="dwtools"))
```

### idxv
DT binary search on multiple keys, also known as *Nth setkey*.  
Creates custom indices for a data.table object. May require lot of memory.  
```{r idxv, results='hide'}
DT = X$SALES
names(DT)
# create some particular indices
Idx = list(
  c("cust_code", "prod_code", "geog_code"),
  c("cust_code", "geog_code", "curr_code"),
  c(2:3)
)
IDX = idxv(DT, Idx)

# binary search on first index # DT[cust_code=="id020" & prod_code==847 & geog_code=="AK"]
DT[CJI(IDX,"id020",847,"AK")]
# binary search on second index # DT[cust_code=="id006" & geog_code=="UT" & curr_code=="NOK"]
DT[CJI(IDX,"id006",TRUE,"UT",TRUE,"NOK")]
# binary search on third index # DT[prod_code==323 & geog_code=="OR"]
DT[CJI(IDX,TRUE,323,"OR")]
```

## Other functions
A brief summary of other functions in the package.  
* `timing` - measure time, nrow in/out, optionally save to db
* `data.equal.data.table` - check if two DT equal, ignore row/col order
* `vwap` - aggregate tick trades data to OHLC including VWAP
* `pkgsVersion` - handy wrapper to compare packages version across libraries
* `anonymize` - anonymization by hashing sensitive data
* `sql.dt` - query data.table using data.table syntax but SQL args and sequence

## License
GPL-3.  
Donations are welcome and will be partially forwarded to dependencies of dwtools.  
[19JRajumtMNU9h9Wvdpsnq13SRdZjfbLeN](https://blockchain.info/address/19JRajumtMNU9h9Wvdpsnq13SRdZjfbLeN)

## Contact
`J.Gorecki@wit.edu.pl`
```{r exit_cleanup, echo=FALSE, results='hide'}
dbDisconnect(conn=sqlite1$conn)
file.remove(c("sqlite1.db","geography.csv"))
```
